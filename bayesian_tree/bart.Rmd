---
output:
  pdf_document:
    fig_crop: false
bibliography: bart_ref.bib
nocite: |
  @cgm98

---

```{r, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, 
                      fig.align = 'center', fig.height = 4.5, fig.width = 6.5)

# libraries and global values
library(tidyverse)
```

\begin{center}
\textbf{Bayesian Additive Tree Regression}

\textit{Possible extension for treatment effect estimation}
\end{center}

\textbf{Introduction to Bayesian Trees}

Classification and regressions trees are widely used supervised learning methods. Their popularity stems from model interpretability and prediction performance. Tree methods partition predictor space into a disjoint set of regions, whose union describes the full predictor space. The partition takes place through splitting rules on $X$ and effectively distributes $X$ to the corresponding region. Each region is then fitted with a model, such as the response variable mean for the $X$s that populate each region. The conditional distribution of the response variable $Y$ given predictor variables $X$ is specified by the tree's splitting rules subject to a chosen fitting criteria, such as minimizing region mean squared error. Regression trees relate predictors to a quantitative response while classification trees relate predictors to a categorical or binary response. This analysis focuses on regression trees.

Terminal nodes, or the partitioned regions, are used for regression tree prediction. Let $y = f(x) + \varepsilon$ be the model describing the relationship between $X$ and $Y$. Notice that we make no assumption on model functional form aside from having additive errors $\varepsilon$ that are distributed $\mathcal{N}(0, \sigma^2)$. $f(x)$ is an unknown function that we want to "learn" from data. Classical regression tree methods take a nonparametric approach to approximate $f(x) = E(y|x)$ through splitting rules. The prediction of $y$ from $x$ is then
$$
\hat{f}(x) = \sum_{b=1}^B\hat{c}_bI(x \in R_b)
$$
where $B$ denotes the number of terminal nodes, $\hat{c}_b$ is the node estimator, and $I(x\in R_b)$ is the indicator function for $x$ being in region $B$. The node estimator $\hat{c}_b$ of $c_b$ is $\text{mean}(y|x \in R_b)$. The collection of node estimates is described by $M = \{\mu_1,...,\mu_B\}$. $f(x)$ describes a single tree $T$ with estimators $M$. Using different notation for $f(x)$ in order to appropriately describe it based on $T$ and $M$, we let $f(x) \Leftrightarrow g(x; T, M)$. We can then describe the set of many trees $\mathcal{G} = \{g(x; T_1, M_1),...,g(x; T_V, M_V)\}$ based on each tree's nodes $M_v = \{\mu_{v1},...,\mu_{vB}\}$ for $v = 1,...,V$.

Moving towards generalized additive models $E(Y|x_1,...,x_p) = \sum_{j=1}^pf_j(x_j)$, the sum-of-trees model becomes $E(Y|X) = \sum_{v=1}^Vg_v(X;T_v, M_v)$. Ensemble methods are currently used to evaluate a set of trees. These methods include bagging, which stochastically generates independent trees to reduce prediction variation by averaging, and boosting, which fits a linear combination of trees for the purpose of explaining variation not captured by the previous fit in order to reduce bias. Random forest methods are also used to evaluate a set of trees and are included in the bagging category. Bayesian tree model averaging produces a semiparametric approach for evaluating a set of trees. Averaging is conducted over a distribution of terminal node posterior samples, generated from a Bayesian tree environment. Bayesian additive regression tree (BART) methods are a direct extension of the single-tree Bayesian tree model [@cgm10].

BART has received recent attention for its causal inference ability when evaluating treatment and control outcomes [@hill10]. In these settings a researcher uses treatment and control samples to construct estimators for quantities of interest like average treatment effects. However, counterfactual problems arise when trying to compute actual treatment effects due to a latent variable structure on observations. The objective of this project (possible paper?) is to extend BART sampling methods by allowing for latent variable predictor space structure. To (possibly) accomplish this objective, I propose a sampling technique that extends BART's existing MH-Gibbs sampler to a EM-MH-Gibbs sampler.

Incorporating an EM algorithm into the BART sampling structure adjusts terminal node posterior form in order to capture uncertainty surrounding unobservable effects. While having full information on treatment and control outcomes negates the need to model uncertainty, in observational settings researchers lack this information. While IV estimation techniques have been widely used, satisfying exclusion restrictions remains subjective. Developing the BART EM-MH-Gibbs estimator contributes to the econometric causal inference field, and statistics field more generally, by producing a technique that lets treatment effect information reveal itself from original data. Specifying informative prior distributions for parameters in Bayesian settings is also a subjective endeavor, but allowing stochastic hyperparameters reduces the effect of prior specification, and this is the approach taken for developing an EM addition to existing BART methods.

A goal is for this model to be used for causal inference in policy settings. For example, policy makers can use historical data to see effects of previous policies and construct future policies that have targeted or more efficient outcomes. Policies seeking nutrient availability interventions in developing areas or livestock health interventions in production systems immediately come to mind. However, the flexibility of this estimator does not constrain its use to any particular research field.

The first step is replicating a BART model, which then allows for estimator extension. A BART model is characterized by a sum-of-trees and approximates $E(Y|X)$ through
$$
f(x) = g(x; T_1, M_1) + g(x; T_2, M_2) + ... + g(x; T_V, M_V). 
$$
A prior on $T$ and $M$ is specified in order to minimize individual tree effects on the full conditional $Y|X$. The result is that each tree accounts for small portions of variation in $Y$. Each portion of variation is modeled through Bayesian backfitting a Markov chain Monte Carlo sample [@ht00] by iteratively constructing residuals on the $i$th iteration and using these residuals to fit the next $i + 1$ iteration. The number of trees $V$ generated for the MCMC sample is user chosen. Independence across priors is assumed, providing 
$$
\begin{aligned}
p((T_1, M_1),...,(T_V,M_V), \sigma) &=\left(\prod_vp(T_v,M_v)\right)p(\sigma) \\[5pt]
&= \left(\prod_vp(M_v|T_v)p(T_v)\right)p(\sigma) \\[5pt]
&= \left(\prod_v\prod_bp(\mu_{bv}|T_v)p(T_v)\right)p(\sigma).
\end{aligned}
$$

The prior $p(T)$ on $T$ is composed of a terminal node splitting part $p_{\text{split}}$ that characterizes the probability of creating a new node from an existing terminal node, and a splitting rule part $p_{\text{rule}}$ that draws a random splitting rule informed by $X$. The probability a terminal node splits is
$$
p_{\text{split}}(\eta, T) = \alpha(1+d_{\eta})^{-\beta},
$$
where $d$ represents the depth of node $\eta$ for tree $T$, and $\alpha \in (0,1)$, $\beta \geq 0$ are constants acting as tuning parameters. Growing a tree is a decreasing function of current node depth, which keeps trees from growing too large. Chipman, George, and McCulloch (1998) (CGM98) tested a grid of $\alpha$ and $\beta$ values and found $\{\alpha, \beta\} = \{0.95, 2\}$ to be effective in keeping individual tree sizes relatively small when incorporated in a sum-of-trees model. The splitting rule $\rho$ for determining $p_{\text{rule}}(\rho|\eta, T)$ is described by the distribution of available predictors, and conditional on the predictors, the available splits defined by the range of each predictor. A splitting rule is then defined as a uniform draw across available predictors followed by a uniform draw of possible split values from the range of the drawn predictor. Splitting rules cannot be constructed in a way that leaves terminal nodes empty. For example, if a binary variable has been previously drawn and used in the tree, it cannot be drawn again.

The prior $p(\mu_{vb}|T_v)$ for $M$ is $\mathcal{N}(0,\sigma_\mu^2)$, where $\sigma_\mu^2 = 0.5/(k\sqrt{V})$. CGM98 recommend $k=2$ as a default so that there is an approximate 95\% prior probability of $E(Y|x)$ being within the interval $(Y_{\text{min}}, Y_{\text{max}})$. CGM98 rescale the response data $Y$ to be in the interval $(-0.5, .5)$ to support ease of prior form and computation. However, rescaling $Y$ is not necessary. $p(\mu_{vb}|T_v)$ would still be chosen to follow $\mathcal{N}(\cdot)$ but $\mu_\mu = 0$ would be replaced with $\mu_\mu = \theta$, which would then either be defined as a constant such as $\bar{Y}$, or $\theta$ would be allowed to vary and follow a specified hyperparameter prior distribution. For the purpose of this part of the analysis, priors are chosen to follow CGM98 defaults.

The prior $p(\sigma)$ on $\sigma$ is chosen as $\nu\lambda(\text{inv}-\chi^2(\nu))$, which is equivalently $\text{inv-gamma}(\nu/2, \nu\lambda/2)$. The shape $\nu$ is chosen between values 3 to 10, with 3 being the recommended default. $\lambda$ is informed by the response variable through $\hat{\sigma}$, which is either estimated as the standard deviation of $Y$ or as the estimated standard deviation of residuals from a linear regression of $Y$ on $X$. $\lambda$ is then chosen such that $P(\sigma < \hat{\sigma}) = q$ for a specified quantile $q$ typically set at 0.9.

The likelihood on $Y$ is specified as $y_{b1},...,y_{bn} \overset{\text{iid}}{\sim} \mathcal{N}(\mu_b, \sigma^2)$ for $b = 1,...,B$. Combining all previous information results in
$$
p(T)p(Y|X,T) \approx p(T)\prod_{b=1}^B\frac{(\nu\lambda)^{\frac{\nu}{2}}}{\pi^{\frac{n_i}{2}}}\sqrt{\frac{a}{n_i + a}}\frac{\Gamma\left(\frac{n_i + \nu}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)}(s_i + t_i + \nu\lambda)^{-\frac{n_i + \nu}{2}}
$$
with $s_i = (n_i - 1)\text{var}(Y_i)$, $t_i = ((n_ia)/(n_i + a))(\bar{y}_i^2)$, and $a = 1/(k\sqrt{m})$.

Generating the posterior chain $\{(T_v, M_v)\}_{v=1}^V$ is accomplished by using a MH-Gibbs step that first accepts a proposal tree $T^*$ for the $T_{(v+1)}$ iteration or keeps the previously generated tree $T_{(v)}$ with probability $\phi$, and then updates with $M_v = \{\mu_{v1},...,\mu_{vB}\}$ draws from $p(\mu_{vb}|T_v)$ depending on the size $B$ of terminal nodes for the accepted tree $T^*$ or $T_{(v)}$ for the current iteration. The proposal densities, or transition kernels, for generating a tree $T^*$ from $T$, $q(T, T^*)$, and generating a tree $T$ from $T^*$, $q(T^*, T)$, are described by the different transition possibilities:
\begin{itemize}
\item[] \textit{grow} - randomly pick a terminal node and assign a splitting rule using $p_{\text{rule}}(\rho|\eta,T)$
\item[] \textit{prune} - randomly pick an internal parent of terminal nodes and collapse the terminal nodes so that the parent is the new terminal node
\item[] \textit{change} - randomly pick an internal parent node and reassign it a splitting rule using $p_{\text{rule}}(\rho|\eta,T)$
\item[] \textit{swap} - randomly pick an internal parent node and swap its splitting rule with the splitting rule of the child nodes
\end{itemize}
After acceptance of the proposal tree or previous tree and the corresponding draws of $M_v$, the residuals are computed for that iteration and are used as the response data in the next iteration. Formally, the MH-Gibbs algorithm is
\begin{itemize}
\item[] Initialize residuals $R_0 = Y$
\item[(i)] Generate $T^*$ from $q(T_{(v)}, T^*)$
\item[(ii)] Take $T_{(v+1)} = T^*$ with probability $\delta(T_{(v)}, T^*)$\footnote{This is accomplished by a Bernoulli draw with probability $\delta$ where acceptance occurs if the draw is 1.}, else $T_{(v+1)}=T_{(v)}$
\begin{itemize}
\item[] where $\delta(T_{(v)}, T^*) = \text{min}\left\{\frac{q(T^*, T_{(v)})}{q(T_{(v)}, T^*)}\frac{p(R_v|X,T^*)}{p(R_v|X,T_{(v)})}\frac{p(T^*)}{p(T_{(v)})}, 1\right\}$
\end{itemize}
\item[(iii)] Draw $M_v$ per $B \in T^*$ or $T_{(v)}$
\item[(iv)] Compute $R_{v+1} = Y - \sum_{l\neq v+1}g(X;T_l, M_l)$
\item[(v)] After a chain of size $V$ has been generated, draw $\sigma$
\end{itemize}

BART replication proceeds by following Hill (2010), who generates BART estimates for generated data using the \texttt{BayesTree} R package developed by Chipman, George, and McCulloch. For variables $Y$, $Z$, and $X$, the data generating process follows:
\begin{itemize}
\item[] $X | Z = 1 \sim \mathcal{N}(40, 10^2)$
\item[] $Y_{(Z=1)} | X \sim \mathcal{N}(90 + e^{0.06X}, 1)$
\item[] $X | Z = 0 \sim \mathcal{N}(20, 10^2)$
\item[] $Y_{(Z=0)} | X \sim \mathcal{N}(72 + 3\sqrt{X}, 1)$
\end{itemize}

Following the above algorithm in CGM98, BART estimates are developed from scratch in R\footnote{Code is appended to the end of this document.}. The plot below provides fitted estimates from one BART chain averaged over posterior draws and successfully replicates the plot in Figure 2 on page 6 of Hill (2010) showing single-tree BART estimates\footnote{Replicated up to random perturbations from the data generation process.}.

```{r, include=FALSE}
source('bart.R')
```

```{r, echo=FALSE}
bart_plot()
```

\textbf{EM Extension to MCMC Bayesian Tree Posterior Sampling}

The EM extension to CGM98's MH-Gibbs algorithm removes the conditioning on Hill's (2010) treatment variable $Z = \{0,1\}$. This approach specifies a latent structure on $Y$. The original specification $y = f(x, z) + \varepsilon$ is respecified as $y = f(x, \omega) + \varepsilon$ where $\omega$ represents a $\mathcal{N}(\theta_\mu, \sigma_\mu^2)$ unobservable effect of $Z$ on $Y$ through $X$. The latent variable $\omega$ is truncated at some value $c$. In order to not introduce bias by directly choosing the truncation point, $c$ is also specified as a parameter with an appropriate prior distribution $p(c)$. This creates an EM step in the existing MH-Gibbs algorithm. The likelihood of $Y$ is augmented to reflect a random $\omega$, $p(Y|X, T, \omega)p(\omega)p(c)$. The "complete" likelihood on $Y$ can be computed using additional information from an EM procedure that iteratively maximizes the expectation of latent variable structure $Y$ with respect to $\theta_\mu$. This is incorporated in the MH step of generating trees, whose first split variable assignment depends on $\omega$. It is the occurance of this first splitting variable in the generated chain of trees that allows evaluation of treatment effect estimators.

With $Y$ following a normal distribution, censoring $Y$ at some value $c$ results in the unobservable variable $Z$ following a truncated normal disribution. Maximizing the complete likelihood in $Y$ with respect to $\theta_\mu$ is completed by iteratively altering the FOC expresion
$$
\hat{\theta} = \frac{m\bar{y} + (n - m)E(Z)}{n},
$$
where $n$ represents the full sample size and $m$ represents the truncated portion of the sample size. The expecation of $Z$ is with respect to the conditional density $Z|\theta, X$ and is characterized by the truncated normal density $\phi(c - \theta)/(1 - \Phi(c - \theta))$, where $\phi$ and $\Phi$ represent normal disribution and density functions, respectively. The full characerization becomes
$$
\hat{\theta}^{(j+1)} = \frac{m}{n}\bar{y} + \frac{(n - m)}{n}\left(\hat{\theta}^{(j)} + \frac{\phi(c - \hat{\theta}^{(j)})}{1 - \phi(c - \hat{\theta}^{(j)})}\right)
$$
and continues in an iterative process until a specified tolerance is met.

The EM implementation in the MH algorith proceeds as follows.
\begin{itemize}
\item[(i)] Generate a sample $Y_s$ of size $n$ with replacement and draw $c \sim \mathcal{U}(\text{min}(Y_s), \text{max}(Y_s))$
\item[(ii)] Given $\{\bar{y}, c, \theta\} = \{\bar{y}_s, c_s, \theta_0\}$
\item[(iii)] Update $\hat{\theta}^{(j+1)}$ and repeat until convergence at $\theta_\mu^*$
\item[(iv)] Generate $\omega_s \sim \mathcal{N}(\theta_\mu^*, \sigma_\mu^2)$ of size $n$
\item[(v)] Compute $\psi = \frac{1}{n}\sum_{i=1}^n1(\omega_s> \bar{y})$
\item[(vi)] Create vector $Z$ and set $Z_i = 1$ with probability $\psi$, else $Z_i = 0$
\end{itemize}

The EM algorithm above is repeated for each tree generated by the MH-Gibbs step. Every tree action  (\textit{grow, prune, change, swap}) is then informed by a new EM result, fully allowing the data to "speak for itself" without observing a treatment variable $Z$. The MH-Gibbs algorithm for each tree is largely unchanged in its description above. The only change is factoring the priors on $\omega$ and $c$ into the likelihood computation of the MH step, which changes from $p(Y|X, T)$ to $p(Y|X, T, \omega)p(\omega)p(c)$. Setting a seed in the random number generator and using the same $(Y, X)$ data for the BART replication plot above, the EM prediction results are plotted below (treating $Z$ as completely unobservable) for a tree chain size of 2000.


```{r, include=FALSE}
source('bayesian_tree_EM.R')
```

```{r, echo=FALSE}
EM_ext_plot()
```

\textbf{References}

<div id="refs"></div>

\newpage 
``` {r, child='Rscript_bayesian_tree_EM.Rmd'}
```
